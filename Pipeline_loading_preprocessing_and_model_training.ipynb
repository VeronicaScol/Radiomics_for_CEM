{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b49d7127-cc96-4294-8be7-10aaa9469e47",
   "metadata": {},
   "source": [
    "# Modelling pipeline\n",
    "## Preprocessing + feature selection + model fitting/ evaluation\n",
    "\n",
    "### Notebook flow overview:\n",
    "1) Loads train/test CSVs and splits into X/y.\n",
    "2) Preprocesses features (fit on train, apply to test) via utils.\n",
    "3) For each (model, feature-selection strategy) pair:\n",
    "   - selects features on TRAIN ONLY\n",
    "   - trains the model on selected TRAIN features\n",
    "   - picks an optimal threshold on train probabilities\n",
    "   - evaluates test-set metrics with bootstrap CIs\n",
    "   - computes confusion matrix (raw + normalized)\n",
    "4) Outputs two comparison tables:\n",
    "   - metrics_comparison.csv\n",
    "   - feature_overlap_jaccard.csv\n",
    "\n",
    "Requirements:\n",
    "- feature_selection_utils.py must be importable (same folder or PYTHONPATH)\n",
    "- utils_refactored.py must be importable\n",
    "- xgboost installed (for XGBClassifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a252f31-532f-48e1-9c15-e85ded896f64",
   "metadata": {},
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11aff68-2efc-4c8a-bd17-8ed0f7fc5ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, Tuple, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# local utilities\n",
    "from radiomics_pipeline.utils import (\n",
    "    preprocessing_train,\n",
    "    preprocessing_test,\n",
    "    predict_proba_1,\n",
    "    get_optimal_threshold,\n",
    "    get_stats_with_ci,\n",
    ")\n",
    "\n",
    "# feature selection utilities (your module)\n",
    "import feature_selection_utils as fsu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b929f15c-b5fa-4239-b7fe-6b3aec2d5566",
   "metadata": {},
   "source": [
    "# HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ebe43e-1962-4909-9140-6b66271f4c37",
   "metadata": {},
   "source": [
    "### Loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a763cbd-6954-45ad-a332-061b2db2fac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a structured container for everything produced by one experiment run\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    model_name: str #<---------Identifies the model\n",
    "    selector_name: str #<---------Ientifies the feature selection method\n",
    "    selected_features: List[str] #<--------------Human readable summary\n",
    "    support_mask: np.ndarray #<---------Bool array; machine safe feature selection\n",
    "    optimal_threshold: float\n",
    "    confusion_matrix: np.ndarray\n",
    "    confusion_matrix_norm: np.ndarray\n",
    "    metrics_test: pd.DataFrame\n",
    "    extra: Dict[str, Any]\n",
    "\n",
    "\n",
    "def load_feature_csv(\n",
    "    train_csv: str,\n",
    "    test_csv: str,\n",
    "    outcome_col: str = \"outcome\",\n",
    "    drop_cols: Tuple[str, ...] = (\"mask_name\", \"outcome\"),\n",
    ") -> Tuple[pd.DataFrame, np.ndarray, pd.DataFrame, np.ndarray]:\n",
    "    \"\"\"Load train/test CSVs and split into X/y.\"\"\"\n",
    "    df_train = pd.read_csv(train_csv)\n",
    "    y_train = df_train[outcome_col].to_numpy()\n",
    "    X_train = df_train.drop(list(drop_cols), axis=1, errors=\"ignore\")\n",
    "\n",
    "    df_test = pd.read_csv(test_csv)\n",
    "    y_test = df_test[outcome_col].to_numpy()\n",
    "    X_test = df_test.drop(list(drop_cols), axis=1, errors=\"ignore\")\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068f2858-c788-45ee-8478-19382d9fdb26",
   "metadata": {},
   "source": [
    "## Ranking:\n",
    "- To assist with RFECV feature number mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fa3078-0c54-4afb-8323-959a26f6cbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _top_n_from_ranking(ranking: np.ndarray, n: int) -> np.ndarray:\n",
    "    \"\"\"Convert a sklearn-style ranking_ array into a boolean mask selecting the top-n features.\n",
    "\n",
    "    ranking_: 1 is best, larger is worse.\n",
    "    \"\"\"\n",
    "    order = np.argsort(ranking)\n",
    "    keep_idx = order[:n]\n",
    "    mask = np.zeros_like(ranking, dtype=bool)\n",
    "    mask[keep_idx] = True\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4b34e6-09c4-425c-a397-0070535394cd",
   "metadata": {},
   "source": [
    "## Feature selection helpers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66930fba-5c30-4e2f-b6ac-597e3941baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(\n",
    "    selector_name: str,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: np.ndarray,\n",
    "    model,\n",
    "    top_n: int = 10,\n",
    ") -> Tuple[List[str], np.ndarray, Dict[str, Any]]:\n",
    "    \"\"\"Dispatch to a feature-selection strategy.\n",
    "\n",
    "    Returns:\n",
    "        selected_features: list of column names\n",
    "        support_mask: boolean mask aligned to X_train.columns\n",
    "        extra: metadata (e.g., best_k, cv scores)\n",
    "    \"\"\"\n",
    "    extra: Dict[str, Any] = {}\n",
    "\n",
    "    if selector_name == \"anova_cv\":\n",
    "        # SelectKBest (ANOVA by default) where k is tuned by CV performance of the model.\n",
    "        selected, best_k, best_score, mask = fsu.filter_method_cv(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            model=model,\n",
    "            return_support=True,\n",
    "        )\n",
    "        extra.update({\"best_k\": best_k, \"best_score\": best_score})\n",
    "        return selected, mask.astype(bool), extra\n",
    "\n",
    "    if selector_name == \"rfe_no_cv_top10\":\n",
    "        selected, mask = fsu.rfe_no_cv(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            n_features=top_n,\n",
    "            estimator=model,\n",
    "            return_support=True,\n",
    "        )\n",
    "        return selected, mask.astype(bool), extra\n",
    "\n",
    "    if selector_name == \"rfecv_rank_top10\":\n",
    "        # RFECV is used to rank features using CV; final training uses top-N by ranking.\n",
    "        # This satisfies the requirement: \"RFECV should select the top 10 features\".\n",
    "        _selected, best_n, best_score, _mask, selector = fsu.rfe_with_cv(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            estimator=model,\n",
    "            # ensure it can evaluate down to 10 features\n",
    "            min_features_to_select=top_n,\n",
    "            n_splits=10,\n",
    "            scoring=\"roc_auc\",\n",
    "            return_support=True,\n",
    "        )\n",
    "        ranking = getattr(selector, \"ranking_\", None)\n",
    "        if ranking is None:\n",
    "            raise RuntimeError(\"RFECV selector did not expose ranking_.\")\n",
    "\n",
    "        mask = _top_n_from_ranking(ranking, top_n)\n",
    "        selected = list(X_train.columns[mask])\n",
    "        extra.update({\"rfecv_best_n_features\": best_n, \"rfecv_best_score\": best_score, \"rfecv\": selector})\n",
    "        return selected, mask.astype(bool), extra\n",
    "\n",
    "    if selector_name == \"embedded_method\":\n",
    "        # For tree/boosting models\n",
    "        selected, mask = fsu.embedded_method(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            n_features=top_n,\n",
    "            model=model,\n",
    "            return_support=True,\n",
    "        )\n",
    "        return selected, mask.astype(bool), extra\n",
    "\n",
    "    if selector_name == \"lasso_logregcv\":\n",
    "        # L1-logistic CV selector (its own model inside a pipeline). Returns selected features.\n",
    "        selected, best_C, n_selected, mask, pipe = fsu.embedded_l1_logregcv(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            return_support=True,\n",
    "        )\n",
    "        # If it selects more than top_n, we keep all by design; project requirement did not\n",
    "        # require forcing lasso to 10. If you *do* want exactly 10, say so.\n",
    "        extra.update({\"best_C\": best_C, \"n_selected\": n_selected, \"pipe\": pipe})\n",
    "        return selected, mask.astype(bool), extra\n",
    "\n",
    "    raise ValueError(f\"Unknown selector_name: {selector_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba325798-2a0f-4d60-b72c-8f46d73ca43c",
   "metadata": {},
   "source": [
    "# Models\n",
    "## Fitting and evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f84b27-49a9-4c2a-b865-6655e39e70ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_evaluate(\n",
    "    model,\n",
    "    X_train_sel: pd.DataFrame,\n",
    "    y_train: np.ndarray,\n",
    "    X_test_sel: pd.DataFrame,\n",
    "    y_test: np.ndarray,\n",
    "    label: str,\n",
    "    nsamples_ci: int = 2000,\n",
    ") -> Tuple[float, np.ndarray, np.ndarray, pd.DataFrame]:\n",
    "    \"\"\"Fit model, pick threshold on train, evaluate test metrics and confusion matrices.\"\"\"\n",
    "    m = clone(model)\n",
    "    m.fit(X_train_sel, y_train)\n",
    "\n",
    "    proba_train = predict_proba_1(m, X_train_sel)\n",
    "    proba_test = predict_proba_1(m, X_test_sel)\n",
    "\n",
    "    thr = float(get_optimal_threshold(y_train, proba_train))\n",
    "\n",
    "    # Metrics with CI strings (already includes thresholded metrics internally)\n",
    "    _, df_metrics = get_stats_with_ci(y_test, proba_test, label, thr, nsamples=nsamples_ci)\n",
    "\n",
    "    # Confusion matrices on test\n",
    "    y_test_pred = (np.asarray(proba_test) > thr).astype(int)\n",
    "    cm = confusion_matrix(y_test, y_test_pred, labels=[0, 1])\n",
    "\n",
    "    # Normalized per true class (rows sum to 1). Guard against division-by-zero.\n",
    "    row_sums = cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.divide(cm, row_sums, out=np.zeros_like(cm, dtype=float), where=row_sums != 0)\n",
    "\n",
    "    # Add confusion matrix values into the metrics row for easy side-by-side comparison.\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    df_metrics = df_metrics.copy()\n",
    "    df_metrics[\"TN\"] = int(tn)\n",
    "    df_metrics[\"FP\"] = int(fp)\n",
    "    df_metrics[\"FN\"] = int(fn)\n",
    "    df_metrics[\"TP\"] = int(tp)\n",
    "    # normalized entries\n",
    "    df_metrics[\"TN_rate\"] = float(cm_norm[0, 0])\n",
    "    df_metrics[\"FP_rate\"] = float(cm_norm[0, 1])\n",
    "    df_metrics[\"FN_rate\"] = float(cm_norm[1, 0])\n",
    "    df_metrics[\"TP_rate\"] = float(cm_norm[1, 1])\n",
    "\n",
    "    return thr, cm, cm_norm, df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f9e595-c217-4758-830f-e97086bcbfda",
   "metadata": {},
   "source": [
    "## Model specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48baf21-fd77-493f-83c2-b1d3844d6f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_specs(random_state: int = 27) -> List[Tuple[str, Any]]:\n",
    "    \"\"\"Define the base models used in experiments.\"\"\"\n",
    "    specs: List[Tuple[str, Any]] = []\n",
    "\n",
    "    specs.append((\n",
    "        \"logreg\",\n",
    "        LogisticRegression(\n",
    "            solver=\"liblinear\",\n",
    "            max_iter=5000,\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=random_state,\n",
    "        ),\n",
    "    ))\n",
    "\n",
    "    specs.append((\n",
    "        \"svm_linear\",\n",
    "        SVC(\n",
    "            kernel=\"linear\",\n",
    "            probability=True,\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=random_state,\n",
    "        ),\n",
    "    ))\n",
    "\n",
    "    specs.append((\n",
    "        \"random_forest\",\n",
    "        RandomForestClassifier(\n",
    "            min_samples_leaf=8,\n",
    "            random_state=random_state,\n",
    "            class_weight=\"balanced\",\n",
    "        ),\n",
    "    ))\n",
    "\n",
    "    specs.append((\n",
    "        \"xgb\",\n",
    "        xgb.XGBClassifier(\n",
    "            use_label_encoder=False,\n",
    "            colsample_bytree=1,\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            nthread=4,\n",
    "            scale_pos_weight=1,\n",
    "            seed=random_state,\n",
    "        ),\n",
    "    ))\n",
    "\n",
    "    return specs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16b608f-aa40-4d4c-be31-1b2a0a3f87f1",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "## Runs all the model- feature selection combos specified and records results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048b30ee-6a2e-4ef0-9d0b-ce0441f03f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: np.ndarray,\n",
    "    top_n: int = 10,\n",
    "    nsamples_ci: int = 2000,\n",
    ") -> Tuple[List[ExperimentResult], pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Run all configured experiments and return results plus comparison tables.\"\"\"\n",
    "\n",
    "    # 1) Preprocess (fit on train, apply to test)\n",
    "    mean_std, var_selector, to_drop, X_train_p = preprocessing_train(X_train)\n",
    "    X_test_p = preprocessing_test(X_test, mean_std, var_selector, to_drop)\n",
    "\n",
    "    # 2) Define model + selector grid\n",
    "    models = build_model_specs(random_state=27)\n",
    "\n",
    "    selectors_for_all = [\n",
    "        \"anova_cv\",          # ANOVA SelectKBest with CV-tuned k\n",
    "        \"rfe_no_cv_top10\",   # RFE selects exactly top 10\n",
    "        \"rfecv_rank_top10\",  # RFECV ranks with CV, then keep top 10\n",
    "    ]\n",
    "\n",
    "    # Additional selectors that are more model-appropriate (optional but useful)\n",
    "    # We'll include embedded importance for tree/boosting models.\n",
    "    tree_embedded_selector = \"embedded_method\"\n",
    "\n",
    "    # L1 logistic CV selector (its own embedded model). We'll run it as a separate experiment.\n",
    "    lasso_selector = \"lasso_logregcv\"\n",
    "\n",
    "    experiments: List[Tuple[str, Any, str]] = []\n",
    "\n",
    "    for model_name, model in models:\n",
    "        # Run ANOVA + both RFE strategies for every model as requested.\n",
    "        for sel in selectors_for_all:\n",
    "            experiments.append((model_name, model, sel))\n",
    "\n",
    "        # Also run embedded selection for tree/boosting models to compare.\n",
    "        if model_name in {\"random_forest\", \"xgb\"}:\n",
    "            experiments.append((model_name, model, tree_embedded_selector))\n",
    "\n",
    "    # Add lasso-style embedded selection (using LogisticRegressionCV with L1)\n",
    "    experiments.append((\"lasso_logregcv\", LogisticRegression(\n",
    "        solver=\"liblinear\",\n",
    "        max_iter=5000,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=27,\n",
    "    ), lasso_selector))\n",
    "\n",
    "    results: List[ExperimentResult] = []\n",
    "\n",
    "    for model_name, model, selector_name in experiments:\n",
    "        selected, mask, extra = select_features(\n",
    "            selector_name,\n",
    "            X_train_p,\n",
    "            y_train,\n",
    "            model,\n",
    "            top_n=top_n,\n",
    "        )\n",
    "\n",
    "        X_train_sel = X_train_p.loc[:, mask]\n",
    "        X_test_sel = X_test_p.loc[:, mask]\n",
    "\n",
    "        label = f\"{model_name} | {selector_name}\"\n",
    "        thr, cm, cm_norm, df_metrics = fit_and_evaluate(\n",
    "            model,\n",
    "            X_train_sel,\n",
    "            y_train,\n",
    "            X_test_sel,\n",
    "            y_test,\n",
    "            label=label,\n",
    "            nsamples_ci=nsamples_ci,\n",
    "        )\n",
    "\n",
    "        results.append(ExperimentResult(\n",
    "            model_name=model_name,\n",
    "            selector_name=selector_name,\n",
    "            selected_features=selected,\n",
    "            support_mask=mask,\n",
    "            optimal_threshold=thr,\n",
    "            confusion_matrix=cm,\n",
    "            confusion_matrix_norm=cm_norm,\n",
    "            metrics_test=df_metrics,\n",
    "            extra=extra,\n",
    "        ))\n",
    "\n",
    "    # 3) Comparison tables\n",
    "    metrics_table = pd.concat([r.metrics_test for r in results], axis=0)\n",
    "\n",
    "    # Feature overlap table (Jaccard similarity)\n",
    "    names = [f\"{r.model_name}|{r.selector_name}\" for r in results]\n",
    "    jacc = pd.DataFrame(index=names, columns=names, dtype=float)\n",
    "    masks = [r.support_mask.astype(bool) for r in results]\n",
    "\n",
    "    for i in range(len(results)):\n",
    "        for j in range(len(results)):\n",
    "            inter = np.logical_and(masks[i], masks[j]).sum()\n",
    "            union = np.logical_or(masks[i], masks[j]).sum()\n",
    "            jacc.iloc[i, j] = (inter / union) if union else 0.0\n",
    "\n",
    "    return results, metrics_table, jacc\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train_csv\", required=True)\n",
    "    parser.add_argument(\"--test_csv\", required=True)\n",
    "    parser.add_argument(\"--top_n\", type=int, default=10)\n",
    "    parser.add_argument(\"--nsamples_ci\", type=int, default=2000)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_feature_csv(args.train_csv, args.test_csv)\n",
    "\n",
    "    results, metrics_table, jaccard = run_experiments(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        top_n=args.top_n,\n",
    "        nsamples_ci=args.nsamples_ci,\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== Test-set metrics (with CI strings + confusion matrices) ===\")\n",
    "    print(metrics_table)\n",
    "\n",
    "    print(\"\\n=== Feature-set overlap (Jaccard) ===\")\n",
    "    print(jaccard)\n",
    "\n",
    "    metrics_table.to_csv(\"metrics_comparison_v2.csv\")\n",
    "    jaccard.to_csv(\"feature_overlap_jaccard_v2.csv\")\n",
    "    print(\"\\nSaved: metrics_comparison_v2.csv, feature_overlap_jaccard_v2.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
