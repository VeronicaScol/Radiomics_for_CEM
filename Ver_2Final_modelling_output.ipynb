{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cef0bafb-6398-4400-8573-da1130970477",
   "metadata": {},
   "source": [
    "# RUN EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe44ede-efd8-40fd-8e00-a2c9902dc28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"python:\", sys.executable)\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"pandas:\", pd.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85766303-6a75-46b2-8787-175504861dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost\n",
    "!pip install -U xgboost matplotlib shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7346bc5-82d3-48a3-a50a-0868b94edb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a303e05-d882-49a6-be1e-74479eb12e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_pipeline_utils import load_feature_csv, run_experiments\n",
    "\n",
    "# Update these paths to point to where your CSV files are actually located\n",
    "# For example, if they're in your current working directory:\n",
    "train_csv = r\"C:\\Users\\albav\\Desktop\\3004 Project\\train_merged_LE_RE.csv\"\n",
    "test_csv = r\"C:\\Users\\albav\\Desktop\\3004 Project\\external_test.csv\"\n",
    "\n",
    "# Or provide the full correct path to your files:\n",
    "# train_csv = r\"C:\\correct\\path\\to\\train_merged_LE_RE.csv\"\n",
    "# test_csv = r\"C:\\correct\\path\\to\\test_merged_LE_RE.csv\"\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_feature_csv(train_csv, test_csv)\n",
    "\n",
    "results, metrics_table, jaccard = run_experiments(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    top_n=10,\n",
    "    nsamples_ci=2000 #<----------------For \"fast run,\" change it to 200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a898f640-d464-4c4c-85ff-b2b81c4220f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2360098-bab1-4208-86f2-db95f8daa04d",
   "metadata": {},
   "source": [
    "# Results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb38ed9-364d-4ef1-ab7f-6e9598efdaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(metrics_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc30b9db-a7da-4350-afd8-89a4bc9c5843",
   "metadata": {},
   "source": [
    "# Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e4cd5-2f5d-4452-8be8-a0278734cdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(results)\n",
    "#4-8 ----> sanity run\n",
    "#10-20 ------> full experiment set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b7dbfe-272d-4027-a3c3-a3089f2d7104",
   "metadata": {},
   "source": [
    "# Information per model\n",
    "- Change results[0] to inspect the different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db54445d-482d-4091-a760-b894716cb625",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataclass fields\n",
    "res = results[0] #<------------change number here to view different model.\n",
    "print(\"Model:\", res.model_name)\n",
    "print(\"Selector:\", res.selector_name)\n",
    "print(\"Optimal threshold:\", res.optimal_threshold)\n",
    "\n",
    "print(\"\\nSelected features:\")\n",
    "print(\"\\n\".join(res.selected_features))\n",
    "\n",
    "print(\"\\nMetrics:\")\n",
    "display(res.metrics_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875916ce-80cc-4eba-b426-d3dda32aa92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.__dict__.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43555e96-9b20-4870-b186-e2918c2b4fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(res.extra))\n",
    "print(res.extra)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257356c2-e928-4078-a84f-fdbb55de76c3",
   "metadata": {},
   "source": [
    "# Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a0102-1d9a-4bd8-9781-813c43178986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "for res in results:\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    ConfusionMatrixDisplay(\n",
    "        confusion_matrix=res.confusion_matrix_norm,\n",
    "        display_labels=[0, 1]\n",
    "    ).plot(ax=ax, values_format=\".2f\")\n",
    "    ax.set_title(f\"{res.model_name} | {res.selector_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e7bbce-5209-4c70-9a10-912affc3f2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fbd6e3-5294-4392-88c2-09b73cbefa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f90cc-d234-4baf-bcb6-1607e029bbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import shap\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 1) choose which run to explain\n",
    "res = results[0]  # change index to explain a different model/selector\n",
    "\n",
    "# 2) build selected-feature matrix\n",
    "X_train_sel = X_train[res.selected_features].copy()\n",
    "\n",
    "# 3) refit a stable logistic regression on those selected features\n",
    "clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lr\", LogisticRegression(max_iter=5000, solver=\"liblinear\"))\n",
    "])\n",
    "clf.fit(X_train_sel, y_train)\n",
    "\n",
    "# 4) SHAP background (keeps it manageable)\n",
    "X_bg = shap.utils.sample(X_train_sel, 100, random_state=27)\n",
    "\n",
    "# 5) explain predict_proba (binary classification -> 2 classes)\n",
    "explainer = shap.Explainer(clf.predict_proba, X_bg)\n",
    "shap_values = explainer(X_train_sel)\n",
    "\n",
    "# 6) Robustly extract \"class 1\" SHAP values and wrap into an Explanation object\n",
    "vals = shap_values.values\n",
    "base = shap_values.base_values\n",
    "\n",
    "print(\"shap_values.values shape:\", vals.shape)\n",
    "\n",
    "# vals can be (n_samples, n_features, n_classes) OR (n_samples, n_features)\n",
    "if vals.ndim == 3:\n",
    "    shap_class1 = vals[:, :, 1]\n",
    "    if np.ndim(base) == 2:\n",
    "        base_class1 = base[:, 1]\n",
    "    else:\n",
    "        base_class1 = base\n",
    "else:\n",
    "    shap_class1 = vals\n",
    "    base_class1 = base\n",
    "\n",
    "sv1 = shap.Explanation(\n",
    "    values=shap_class1,\n",
    "    base_values=base_class1,\n",
    "    data=X_train_sel.values,\n",
    "    feature_names=list(X_train_sel.columns)\n",
    ")\n",
    "\n",
    "# 7) Plots\n",
    "shap.plots.beeswarm(sv1, max_display=15)\n",
    "\n",
    "idx = np.random.RandomState(27).randint(0, X_train_sel.shape[0])\n",
    "print(\"case index:\", idx)\n",
    "shap.plots.waterfall(sv1[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58efaf3d-ecb5-4d06-b805-5907c9c0d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHAP VALUES VANILLE\n",
    "\n",
    "#X100 = shap.utils.sample(reduced_features_train_rfecv, 100, random_state=27) # I am not yet sure what the optimal number for distribution is. Standard (explained in documentation is 100.\n",
    "#explainer_xgb = shap.Explainer(rfecv.estimator_, X100) #This utilises the rfe xgboost with 10 features\n",
    "#shap_values_xgb = explainer_xgb(reduced_features_train_rfecv) #based on training dataset of model, since that is what controls final model architecture\n",
    "#shap.plots.beeswarm(shap_values_xgb, max_display=10)\n",
    "#X100 = shap.utils.sample(reduced_features_train, 100, random_state=27) # I am not yet sure what the optimal number for distribution is. Standard (explained in documentation is 100.\n",
    "#explainer_xgb = shap.Explainer(rfe.estimator_, X100) #This utilises the rfe xgboost with 10 features\n",
    "#shap_values_xgb = explainer_xgb(reduced_features_train) #based on training dataset of model, since that is what controls final model architecture\n",
    "#shap.plots.beeswarm(shap_values_xgb, max_display=len(reduced_features_train))\n",
    "#import random\n",
    "#random_case = random.randint(0, len(reduced_features_train + 1))\n",
    "#print(\"case index: \" + str(random_case))\n",
    "\n",
    "#shap.plots.waterfall(shap_values_xgb[random_case])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad3c754-9c25-4058-a80e-87185c84d29b",
   "metadata": {},
   "source": [
    "XGBoost Models -\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b461085b-5f42-447c-acfe-e139d1b5f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_run_combinations(results):\n",
    "    for i, r in enumerate(results):\n",
    "        print(f\"{i:2d} | {r.model_name:10s} | {r.selector_name}\")\n",
    "\n",
    "list_run_combinations(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3727d7-c0df-47c7-963b-04a4ad806dfb",
   "metadata": {},
   "source": [
    "general SHAP function (XGBoost)\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2ce043-836f-4a8d-99d3-f20db92f0c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "SEED = 27\n",
    "\n",
    "def shap_xgb_for_run(\n",
    "    results,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    model_name: str,\n",
    "    selector_name: str,\n",
    "    *,\n",
    "    background_n: int = 100,\n",
    "    explain_n: int | None = 300,     # keep this smaller for speed\n",
    "    max_display: int = 15,\n",
    "    case_idx: int | None = None,\n",
    "    seed: int = SEED,\n",
    "):\n",
    "    # 1) find the matching run\n",
    "    match = None\n",
    "    for i, r in enumerate(results):\n",
    "        if r.model_name == model_name and r.selector_name == selector_name:\n",
    "            match = (i, r)\n",
    "            break\n",
    "    if match is None:\n",
    "        raise ValueError(f\"No run found for: {model_name} | {selector_name}\")\n",
    "\n",
    "    run_idx, res = match\n",
    "    print(\"Selected run:\", run_idx, \"|\", res.model_name, \"|\", res.selector_name)\n",
    "\n",
    "    # 2) selected feature matrix\n",
    "    X_train_sel = X_train[res.selected_features].copy()\n",
    "\n",
    "    # subset to explain (speed)\n",
    "    if explain_n is not None and explain_n < X_train_sel.shape[0]:\n",
    "        X_explain = shap.utils.sample(X_train_sel, explain_n, random_state=seed)\n",
    "    else:\n",
    "        X_explain = X_train_sel\n",
    "\n",
    "    # 3) refit XGBoost\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=400,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=1.0,\n",
    "        objective=\"binary:logistic\",\n",
    "        random_state=seed,\n",
    "        n_jobs=-1,\n",
    "        eval_metric=\"logloss\",\n",
    "    )\n",
    "    xgb.fit(X_train_sel, y_train)\n",
    "\n",
    "    # 4) SHAP (PermutationExplainer; robust for your SHAP/XGB versions)\n",
    "    X_bg = shap.utils.sample(X_train_sel, min(background_n, X_train_sel.shape[0]), random_state=seed)\n",
    "\n",
    "    explainer = shap.Explainer(\n",
    "        xgb.predict_proba,\n",
    "        X_bg,\n",
    "        algorithm=\"permutation\"\n",
    "    )\n",
    "    sv = explainer(X_explain)\n",
    "\n",
    "    # 5) Convert to \"class 1\" explanation safely\n",
    "    # sv.values is typically (n_samples, n_features, n_classes) for predict_proba\n",
    "    sv1 = shap.Explanation(\n",
    "        values=sv.values[:, :, 1],\n",
    "        base_values=sv.base_values[:, 1] if getattr(sv, \"base_values\", None) is not None else None,\n",
    "        data=sv.data,\n",
    "        feature_names=sv.feature_names\n",
    "    )\n",
    "\n",
    "    # 6) Beeswarm (class 1)\n",
    "    shap.plots.beeswarm(sv1, max_display=max_display, show=False)\n",
    "    ax = plt.gca()\n",
    "    ax.set_title(f\"{model_name} | {selector_name} â€” SHAP beeswarm (global)\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 7) Waterfall (class 1)\n",
    "    if case_idx is None:\n",
    "        case_idx = np.random.RandomState(seed).randint(0, X_explain.shape[0])\n",
    "    print(\"Waterfall case index (within explained set):\", case_idx)\n",
    "    shap.plots.waterfall(sv1[case_idx])\n",
    "\n",
    "    return sv1, res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6278dac-f6e3-4660-aa79-8f11fe3d4112",
   "metadata": {},
   "source": [
    "XGBoost + RFECV\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad781c93-fe05-4859-b4c7-4ffbce636e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_rfecv, res_rfecv = shap_xgb_for_run(\n",
    "    results, X_train, y_train,\n",
    "    model_name=\"xgb\",\n",
    "    selector_name=\"rfecv_rank_top10\",\n",
    "    explain_n=500  # optional; speeds it up\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a12e31e-e049-42c0-8641-a39f0d797faf",
   "metadata": {},
   "source": [
    "XGBoost + RFE (no CV)\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398bb04a-05fb-4795-b22d-478403b5f878",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_rfe, res_rfe = shap_xgb_for_run(\n",
    "    results, X_train, y_train,\n",
    "    model_name=\"xgb\",\n",
    "    selector_name=\"rfe_no_cv_top10\",\n",
    "    explain_n=500  # optional\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6698fd63-0193-4bdb-a015-d17245029f37",
   "metadata": {},
   "source": [
    "XGBoost + ANOVA-CV\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bce6839-204a-4eae-b535-f7a006b46c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_anova, res_anova = shap_xgb_for_run(\n",
    "    results, X_train, y_train,\n",
    "    model_name=\"xgb\",\n",
    "    selector_name=\"anova_cv\",\n",
    "    explain_n=500  # optional; set to 300 if you want faster\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8334d68-e6c5-46ec-af56-492208d3b4ef",
   "metadata": {},
   "source": [
    "XGBoost + Embedded Method\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cefdca-7c23-43c7-81a6-89a8f298e263",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_embedded, res_embedded = shap_xgb_for_run(\n",
    "    results, X_train, y_train,\n",
    "    model_name=\"xgb\",\n",
    "    selector_name=\"embedded_method\",\n",
    "    explain_n=500  # optional; reduce if it gets slow\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5b2cc0-eba8-4927-8984-d260f3f74040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "SEED = 27\n",
    "\n",
    "\n",
    "# -------- helpers --------\n",
    "\n",
    "def _extract_scores_from_extra(res):\n",
    "    \"\"\"\n",
    "    Try to pull test-set scores/probabilities from res.extra if your pipeline stored them.\n",
    "    Returns a 1D array of scores for class 1, or None if not found.\n",
    "    \"\"\"\n",
    "    if not hasattr(res, \"extra\") or res.extra is None or not isinstance(res.extra, dict):\n",
    "        return None\n",
    "\n",
    "    # common key names that pipelines often use\n",
    "    candidate_keys = [\n",
    "        \"y_proba_test\", \"y_pred_proba_test\", \"proba_test\", \"y_prob_test\",\n",
    "        \"y_score_test\", \"scores_test\", \"decision_test\", \"yhat_proba_test\"\n",
    "    ]\n",
    "\n",
    "    for k in candidate_keys:\n",
    "        if k in res.extra:\n",
    "            s = np.asarray(res.extra[k])\n",
    "            # if it's (n,2) take class 1\n",
    "            if s.ndim == 2 and s.shape[1] >= 2:\n",
    "                return s[:, 1]\n",
    "            # if it's (n,) assume already class-1 score\n",
    "            if s.ndim == 1:\n",
    "                return s\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def _fit_model_for_run(model_name: str, seed: int = SEED):\n",
    "    \"\"\"\n",
    "    Build a model matching the model_name labels used in your results.\n",
    "    Returns an estimator that can produce either predict_proba or decision_function.\n",
    "    \"\"\"\n",
    "    if model_name == \"logreg\":\n",
    "        return Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"clf\", LogisticRegression(max_iter=5000, solver=\"liblinear\", random_state=seed))\n",
    "        ])\n",
    "\n",
    "    if model_name == \"svm_linear\":\n",
    "        # LinearSVC does NOT give predict_proba, but it gives decision_function (good for ROC).\n",
    "        return Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"clf\", LinearSVC(random_state=seed))\n",
    "        ])\n",
    "\n",
    "    if model_name == \"random_forest\":\n",
    "        return RandomForestClassifier(\n",
    "            n_estimators=500,\n",
    "            random_state=seed,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    if model_name == \"xgb\":\n",
    "        # Use same settings as your SHAP refit; ROC only needs consistent scores.\n",
    "        return XGBClassifier(\n",
    "            n_estimators=400,\n",
    "            max_depth=3,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=1.0,\n",
    "            objective=\"binary:logistic\",\n",
    "            random_state=seed,\n",
    "            n_jobs=-1,\n",
    "            eval_metric=\"logloss\",\n",
    "        )\n",
    "\n",
    "    if model_name == \"lasso_logregcv\":\n",
    "        return Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"clf\", LogisticRegressionCV(\n",
    "                penalty=\"l1\",\n",
    "                solver=\"saga\",\n",
    "                max_iter=10000,\n",
    "                cv=5,\n",
    "                random_state=seed,\n",
    "                n_jobs=-1\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "    raise ValueError(f\"Unknown model_name: {model_name}\")\n",
    "\n",
    "\n",
    "def _get_test_scores(res, X_train, y_train, X_test, y_test, seed: int = SEED):\n",
    "    \"\"\"\n",
    "    Returns (y_score_test, X_test_sel) where y_score_test is 1D numeric scores for ROC.\n",
    "    Prefers stored scores in res.extra; otherwise refits the model on selected features.\n",
    "    \"\"\"\n",
    "    # 1) Try to use stored scores\n",
    "    s = _extract_scores_from_extra(res)\n",
    "    if s is not None:\n",
    "        return s, None\n",
    "\n",
    "    # 2) Otherwise refit\n",
    "    X_train_sel = X_train[res.selected_features].copy()\n",
    "    X_test_sel = X_test[res.selected_features].copy()\n",
    "\n",
    "    est = _fit_model_for_run(res.model_name, seed=seed)\n",
    "    est.fit(X_train_sel, y_train)\n",
    "\n",
    "    # prefer predict_proba if available; otherwise use decision_function\n",
    "    if hasattr(est, \"predict_proba\"):\n",
    "        proba = est.predict_proba(X_test_sel)\n",
    "        y_score = proba[:, 1]\n",
    "    elif hasattr(est, \"decision_function\"):\n",
    "        y_score = est.decision_function(X_test_sel)\n",
    "    else:\n",
    "        raise RuntimeError(f\"Model {res.model_name} has neither predict_proba nor decision_function\")\n",
    "\n",
    "    return np.asarray(y_score), X_test_sel\n",
    "\n",
    "\n",
    "# -------- main plotting --------\n",
    "\n",
    "def plot_roc_for_all_runs(results, X_train, y_train, X_test, y_test, seed: int = SEED):\n",
    "    \"\"\"\n",
    "    Makes one ROC plot per (model, selector) run in results.\n",
    "    \"\"\"\n",
    "    for i, res in enumerate(results):\n",
    "        y_score, _ = _get_test_scores(res, X_train, y_train, X_test, y_test, seed=seed)\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n",
    "        plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(f\"ROC: {res.model_name} | {res.selector_name} (run {i})\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Run this:\n",
    "plot_roc_for_all_runs(results, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d155931-afff-48a9-8ca0-7928672d7213",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (radiomics)",
   "language": "python",
   "name": "radiomics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
