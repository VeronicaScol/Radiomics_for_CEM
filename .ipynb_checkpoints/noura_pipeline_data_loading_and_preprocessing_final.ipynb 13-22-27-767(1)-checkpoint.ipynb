{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce52ea82-9e2f-4b01-b86c-cadb5538ff03",
   "metadata": {},
   "source": [
    "# PIPELINE DATA LOADING AND PREPROCESSING\n",
    "This notebook documents the data loading, preprocessing of radiomics features, and feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32766547-ad4a-4037-bbf9-1d57fa51fe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shap\n",
    "!pip install xgboost\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import ntpath\n",
    "import sklearn\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "\n",
    "# Now import from radiomics_pipeline\n",
    "from radiomics_pipeline.utils import preprocessing_train, preprocessing_test, get_results, get_ci, get_stats_with_ci, get_ci_for_auc, get_optimal_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983fa9e6-1b3c-48ef-9d86-53a03d0381a6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### STEP 1: Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30995f3-5ec9-4b30-b31a-77ac9345628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load features\n",
    "df_features_train = pd.read_csv(\"\")\n",
    "outcome_train = list(df_features_train[\"outcome\"])\n",
    "df_features_train.drop([\"mask_name\",\"outcome\"], inplace=True, axis=1)\n",
    "df_features_test = pd.read_csv(\"\")\n",
    "outcome_test = list(df_features_test[\"outcome\"])\n",
    "df_features_test.drop([\"mask_name\",\"outcome\"], inplace=True, axis=1)\n",
    "\n",
    "#The below gives the error 'Outcome' (not a column in df features test), this has to be fixed in the merge file itself.\n",
    "\n",
    "#df_features_external = pd.read_csv(\"~/Documents/Features/external_merged_LE_RE.csv\")\n",
    "#outcome_external = list(df_features_test[\"outcome\"])\n",
    "#df_features_external.drop([\"mask_name\",\"outcome\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32f667-9037-46cc-9447-f997ba7ad59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load radiomics (same as above)\n",
    "df_radiomics_train = pd.read_csv(\"\")\n",
    "df_radiomics_test  = pd.read_csv(\"\")\n",
    "\n",
    "# Load clinical\n",
    "clinical_train = pd.read_csv(\"\")\n",
    "clinical_test  = pd.read_csv(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5b5006-3930-487b-ba42-4bbbb5b1256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean strings (good practice)\n",
    "df_radiomics_train[\"mask_name\"] = df_radiomics_train[\"mask_name\"].astype(str).str.strip()\n",
    "df_radiomics_test[\"mask_name\"]  = df_radiomics_test[\"mask_name\"].astype(str).str.strip()\n",
    "\n",
    "clinical_train[\"UM_ID\"] = clinical_train[\"UM_ID\"].astype(str).str.strip()\n",
    "clinical_test[\"UM_ID\"]  = clinical_test[\"UM_ID\"].astype(str).str.strip()\n",
    "\n",
    "# ── KEY FIX: Extract MUMC_XXXX from the full path ──\n",
    "df_radiomics_train[\"patient_id\"] = df_radiomics_train[\"mask_name\"].str.extract(r'(MUMC_\\d+)')[0]\n",
    "df_radiomics_test[\"patient_id\"]  = df_radiomics_test[\"mask_name\"].str.extract(r'(MUMC_\\d+)')[0]\n",
    "\n",
    "# Rename clinical column to match\n",
    "clinical_train = clinical_train.rename(columns={\"UM_ID\": \"patient_id\"})\n",
    "clinical_test  = clinical_test.rename(columns={\"UM_ID\": \"patient_id\"})\n",
    "\n",
    "# Drop any clinical outcome columns (to avoid leakage)\n",
    "outcome_cols = [col for col in clinical_train.columns if col.startswith(\"Final diagnosis_\")]\n",
    "clinical_train = clinical_train.drop(columns=outcome_cols, errors='ignore')\n",
    "clinical_test  = clinical_test.drop(columns=outcome_cols, errors='ignore')\n",
    "\n",
    "# Merge on the extracted patient_id (left join = keep all lesions)\n",
    "df_combined_train = pd.merge(\n",
    "    df_radiomics_train,\n",
    "    clinical_train,\n",
    "    on=\"patient_id\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_clinical\")\n",
    ")\n",
    "\n",
    "df_combined_test = pd.merge(\n",
    "    df_radiomics_test,\n",
    "    clinical_test,\n",
    "    on=\"patient_id\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_clinical\")\n",
    ")\n",
    "\n",
    "# Extract outcomes from radiomics\n",
    "outcome_train = df_combined_train[\"outcome\"].tolist()\n",
    "outcome_test  = df_combined_test[\"outcome\"].tolist()\n",
    "\n",
    "# Drop identifier columns (only keep pure features)\n",
    "drop_cols = [\"mask_name\", \"outcome\", \"patient_id\"]\n",
    "df_features_train = df_combined_train.drop(columns=drop_cols, errors='ignore')\n",
    "df_features_test  = df_combined_test.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "# Quick success check\n",
    "print(\"Combined train shape:\", df_features_train.shape)  # Should be ~ (1811, ~1334+clinical_cols)\n",
    "print(\"Any missing patient_id after extraction?\", df_radiomics_train[\"patient_id\"].isnull().sum())\n",
    "print(\"Example patient_ids from radiomics:\", df_radiomics_train[\"patient_id\"].head(5).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a35626f-bc94-47a1-89a9-6daf151ce87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. How many unique patients in the combined data?\n",
    "print(\"Unique patients in combined train:\", df_combined_train[\"patient_id\"].nunique())\n",
    "\n",
    "# 2. Any NaNs in clinical columns after merge? (should be 0 or very few)\n",
    "clinical_cols = [col for col in df_features_train.columns if not col.startswith(('LE__', 'RE__'))]\n",
    "print(\"\\nMissing values in clinical columns:\")\n",
    "print(df_features_train[clinical_cols].isnull().sum())\n",
    "\n",
    "# 3. Example of merged data (first 3 rows – should show both radiomics + clinical)\n",
    "print(\"\\nFirst 3 rows of combined features (radiomics + clinical):\")\n",
    "print(df_features_train.iloc[:3, :10])          # first 10 columns\n",
    "print(df_features_train.iloc[:3, -10:])         # last 10 columns (likely clinical)\n",
    "\n",
    "# 4. Class balance still makes sense?\n",
    "print(\"\\nClass balance in combined train:\")\n",
    "print(pd.Series(outcome_train).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf50030-45dc-4782-95bc-d5d5210fa706",
   "metadata": {},
   "source": [
    "##### What did I do?\n",
    "- Loaded the merged radiomics files (train_merged_LE_RE.csv and test_merged_LE_RE.csv) these contain ~1320 radiomics features per lesion (LE__ and RE__ prefixed) + mask_name + outcome\n",
    "\n",
    "- Loaded the processed clinical files (combined_clinical_features_train_processed.csv and test version) \n",
    "\n",
    "- mask_name is a full file path (e.g. E:\\MData\\...\\MUMC_1..._CC_L_STRUCT1.mha), while clinical uses clean UM_ID like MUMC_1...\n",
    "\n",
    "- Used regex to extract the patient ID (MUMC_XXXX) from every mask_name path → created patient_id column\n",
    "\n",
    "- Renamed UM_ID → patient_id in clinical\n",
    "\n",
    "- Dropped redundant outcome columns from clinical (Final diagnosis_...) to prevent leakage\n",
    "\n",
    "- Merged radiomics + clinical on patient_id with how=\"left\" → every lesion keeps its row, and gets the clinical info of its patient attached\n",
    "\n",
    "##### Result \n",
    "\n",
    "T- rain shape: (1811, 1334) → 1811 lesions × (radiomics features + clinical features)\n",
    "- Unique patients: 850 → matches the number of patients in clinical file\n",
    "- No missing values in clinical columns after merge\n",
    "- Example rows show radiomics features + Age, Menopause, Cup size, etc. repeating for lesions of the same patient\n",
    "- Class balance still ~50/50 — merge didn’t break anything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e6bccf-0afb-4d15-81c5-b188c390a4ce",
   "metadata": {},
   "source": [
    "### STEP 2: Preprocessing features\n",
    "The preprocessing of the features is present in radiomics_pipeline.utils.\n",
    "\n",
    "It includes:\n",
    "- Normalization\n",
    "- Low variance feature removal (variance below 0.01)\n",
    "- Highly correlated feature removal (Spearman correlation matrix, correlation > 0.85, dropping one feature based on heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27713778-e73f-4608-ae3d-0fbc1ccd9945",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_std, selector, to_drop, decor_dataset_train = preprocessing_train(df_features_train)\n",
    "decor_dataset_test = preprocessing_test(df_features_test, mean_std, selector, to_drop)\n",
    "print(\"features processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c55dc7-e97b-42b9-9cab-0786f85a747c",
   "metadata": {},
   "source": [
    "### STEP 3: Select optimal features \n",
    "#### WRAPPER FEATURE SELECTION: Recursive Feature Elimination with cross-validation\n",
    "The model currently used is XGBClassifier, it is possible to change it to compare different models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5603e0-7b37-4e6f-a529-b163d711866f",
   "metadata": {},
   "source": [
    "#### RECURSIVE FEATURE ELIMINATION (NO CROSS VALIDATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929804cf-0531-4bd3-9b87-ff96e3db2a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model: XGBoost Classifier\n",
    "import xgboost as xgb\n",
    "model = xgb.XGBClassifier(use_label_encoder=False, colsample_bytree=1,\n",
    "                          objective='binary:logistic', eval_metric='logloss', nthread=4, scale_pos_weight=1,\n",
    "                          seed=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a09a397-e8aa-4b29-8496-70d2e7a505dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfe = RFE(estimator=model, n_features_to_select=11, step=1)\n",
    "rfe.fit(decor_dataset_train, outcome_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f843bd-fc21-4867-9bc3-d9dd16532715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL: Extract selected features immediately after fit\n",
    "\n",
    "support = rfe.support_\n",
    "selected_features = decor_dataset_train.columns[support].tolist()\n",
    "\n",
    "print(f\"Selected exactly {len(selected_features)} features (as requested):\")\n",
    "print(selected_features)\n",
    "\n",
    "# Create reduced datasets with ONLY these 10 features\n",
    "reduced_features_train = decor_dataset_train[selected_features].copy()\n",
    "reduced_features_test  = decor_dataset_test[selected_features].copy()\n",
    "\n",
    "print(\"\\nReduced train shape:\", reduced_features_train.shape)   # Should be (1811, 10)\n",
    "print(\"Reduced test shape: \", reduced_features_test.shape)     # Should be (454, 10)\n",
    "print(\"Model expects:\", rfe.estimator_.n_features_in_, \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e181819e-2c43-4026-a385-d1da6790611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────\n",
    "# Predict on TEST – vectorized (fast, no loop needed)\n",
    "# ────────────────────────────────────────────────\n",
    "print(\"Generating probabilities for test set (using exactly 10 features)...\")\n",
    "\n",
    "# Predict all at once\n",
    "all_predictions_test = rfe.estimator_.predict_proba(reduced_features_test)[:, 1]\n",
    "\n",
    "# Save\n",
    "save_dir = os.path.expanduser(\"~/Documents/Features\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "test_path = os.path.join(save_dir, \"probabilities_test.pkl\")\n",
    "\n",
    "with open(test_path, \"wb\") as f:\n",
    "    pickle.dump(all_predictions_test, f)\n",
    "\n",
    "print(f\"Saved test probabilities: {test_path}\")\n",
    "print(f\"Number of test predictions: {len(all_predictions_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feae0bb-faab-439b-a035-6598284de90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────\n",
    "# Predict on TRAIN (for optimal threshold)\n",
    "# ────────────────────────────────────────────────\n",
    "print(\"Generating probabilities for train set (for threshold calculation)...\")\n",
    "\n",
    "all_predictions_train = rfe.estimator_.predict_proba(reduced_features_train)[:, 1]\n",
    "\n",
    "train_path = os.path.join(save_dir, \"probabilities_train.pkl\")\n",
    "\n",
    "with open(train_path, \"wb\") as f:\n",
    "    pickle.dump(all_predictions_train, f)\n",
    "\n",
    "print(f\"Saved train probabilities: {train_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6002334-9759-4582-b75e-9dc2771ebe7a",
   "metadata": {},
   "source": [
    "### Explanation: How the prediction / shape mismatch errors were fixed\n",
    "\n",
    "The main problems were:\n",
    "- RFE selected only 10 features → the final XGBoost model (inside `rfe.estimator_`) only knows how to predict using **exactly 10 columns**\n",
    "- When predicting on test data, rows still contained many more columns (after preprocessing) → `ValueError: \"expected: 10, got XX\"`\n",
    "- `NameError` because `'selected_features'` was not defined when it was needed in the loop\n",
    "\n",
    "Steps taken to resolve the issues:\n",
    "\n",
    "1. Right after fitting RFE, the selected features were extracted using the boolean mask from `rfe.support_`. This produced a list with the **exact names** of the 10 features chosen by RFE (e.g. texture features from LE/RE + possibly Age or other clinical ones).\n",
    "\n",
    "2. Reduced versions of the train and test sets were created using only those 10 selected column names. Both datasets now contain exactly 10 columns — matching what the model was trained to expect.\n",
    "\n",
    "3. The slow row-by-row prediction loop was replaced with **vectorized prediction** on the entire reduced test set at once. This processes all test samples efficiently, eliminates shape mismatches, and removes the need to reshape individual rows.\n",
    "\n",
    "4. Quick print checks were added to verify alignment before prediction: the shape of the reduced test set and the number of features the model expects. Matching numbers confirm that the data and model are compatible.\n",
    "\n",
    "After these changes:\n",
    "- The \"feature shape mismatch\" error no longer occurs\n",
    "- The `NameError` for `selected_features` is resolved\n",
    "- Predictions run correctly and efficiently\n",
    "- The model continues to use **exactly 10 features** (as intended, to align with the paper and prevent overfitting)\n",
    "\n",
    "The same approach was applied to the train set predictions (used for optimal threshold calculation).\n",
    "\n",
    "As a result, the full pipeline — RFE → prediction → threshold → metrics → ROC → SHAP — now executes without interruption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6242ca3-256f-4d0c-84a3-e1a5515ad684",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### RECURSIVE FEATURE ELIMINATION WITH CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776efe7d-80f9-42bf-88cb-73dd45a0d582",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_features_to_select = 1  # Minimum number of features to consider\n",
    "rfecv = RFECV(estimator=model, step=1, cv=StratifiedKFold(10),\n",
    "              scoring='roc_auc',\n",
    "              min_features_to_select=min_features_to_select)\n",
    "rfecv.fit(decor_dataset_train, outcome_train)\n",
    "support = rfecv.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0802bcd-8886-4c6d-b12b-0b2cca0f4c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv.cv_results_[\"mean_test_score\"]\n",
    "rfecv.cv_results_[\"std_test_score\"]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(\n",
    "    range(\n",
    "        min_features_to_select,\n",
    "        min_features_to_select + len(rfecv.cv_results_[\"mean_test_score\"])\n",
    "    ),\n",
    "    rfecv.cv_results_[\"mean_test_score\"]\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Number of selected features\")\n",
    "plt.ylabel(\"Mean CV ROC-AUC\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26863d4a-9315-4965-b945-6a79fcf2231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of features selected\n",
    "support.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e07dc3e-75fa-4fd7-a9b4-a7c0aadbaddd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### FILTER FEATURE SELECTION: ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ae09d6-f7f5-42e0-a4eb-4f8d8bc310f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "# Build pipeline\n",
    "pipe = Pipeline([\n",
    "    (\"anova\", SelectKBest(score_func=f_classif)),\n",
    "    (\"model\", model),\n",
    "])\n",
    "\n",
    "# Grid of k values\n",
    "param_grid = {\n",
    "    \"anova__k\": [5, 10, 15, 20, 30, 40, 45, 50, 60, 70, 80, 90, 100, \"all\"]\n",
    "}\n",
    "\n",
    "# CV strategy\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Grid search\n",
    "gsearch = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit\n",
    "gsearch.fit(decor_dataset_train, outcome_train)\n",
    "\n",
    "# Results\n",
    "print(\"Best k:\", gsearch.best_params_[\"anova__k\"])\n",
    "print(\"Best CV AUC:\", gsearch.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ff6880-c060-4e37-ba98-ce53e9d7fc69",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### EMBEDDED FEATURE SELECTION: LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675db1e7-d0ef-4baf-bef9-021c810bce17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "logit_l1 = LogisticRegressionCV(\n",
    "    penalty=\"l1\",\n",
    "    solver=\"saga\",         \n",
    "    scoring=\"roc_auc\",\n",
    "    cv=10,\n",
    "    max_iter=10000,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit on your training data\n",
    "logit_l1.fit(decor_dataset_train, outcome_train)\n",
    "\n",
    "# Selected features\n",
    "coef = logit_l1.coef_.ravel()\n",
    "n_features_selected = (coef != 0).sum()\n",
    "\n",
    "print(\"Best C:\", logit_l1.C_[0])\n",
    "print(f\"Features selected: {n_features_selected} out of {decor_dataset_train.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a17995-6baa-4e43-97be-380a30687bc2",
   "metadata": {},
   "source": [
    "#### MODEL TRAINING : XGBOOST + EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99d10fb-d978-4347-972a-951d9ba7f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the selected features only\n",
    "filtered_col = np.extract(support, np.array(decor_dataset_test.columns))\n",
    "reduced_features_test = decor_dataset_test[filtered_col]\n",
    "reduced_features_train = decor_dataset_train[filtered_col]\n",
    "print(\"features processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924c5a85-f9df-44cf-b060-8d57df31a68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_images_test = list(df_features_test.index)\n",
    "\n",
    "#predict case by case for test\n",
    "\n",
    "all_predictions = []\n",
    "for i,index in enumerate(path_images_test):\n",
    "        temp_proba = rfe.estimator_.predict_proba(reduced_features_test.iloc[i].values.reshape(1, -1)) #look into what estimator_ does\n",
    "        all_predictions.append(temp_proba)\n",
    "\n",
    "all_predictions_test = np.array([prediction[0][1] for prediction in all_predictions])\n",
    "\n",
    "file = open(os.path.expanduser(\"~/Documents/Features\") + \"/\" + ntpath.basename(\"/probabilities_test\").split(\".\")[0]+\".pkl\", \"wb\")\n",
    "pickle.dump(all_predictions_test, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8efc490-7287-4808-bc17-e14605825a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DETERMINING THE OPTIMAL THRESHHOLD FOR CLASSIFICATION BOUNDARY\n",
    "## This is not for evaluation since it is on the training dataset.\n",
    "\n",
    "path_images_train = list(df_features_train.index)\n",
    "\n",
    "#predict case by case for train to obtain optimal threshhold\n",
    "\n",
    "all_predictions = []\n",
    "for i,index in enumerate(path_images_train):\n",
    "        temp_proba = rfe.estimator_.predict_proba(reduced_features_train.iloc[i].values.reshape(1, -1)) #look into what estimator_ does\n",
    "        all_predictions.append(temp_proba)\n",
    "\n",
    "all_predictions_train = np.array([prediction[0][1] for prediction in all_predictions])\n",
    "\n",
    "file = open(os.path.expanduser(\"~/Documents/Features\") + \"/\" + ntpath.basename(\"/probabilities_train\").split(\".\")[0]+\".pkl\", \"wb\")\n",
    "pickle.dump(all_predictions_train, file)\n",
    "file.close()\n",
    "\n",
    "#Determine optimal threshhold\n",
    "\n",
    "optimal_threshold = get_optimal_threshold(outcome_train, all_predictions_train) # (true_outcome, predictions): to obtain a good threshold based on the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcf5f56-76b1-490c-959e-979f46b6d534",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce94f233-9258-4d5f-ad2d-45025cd6c7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_test_array = np.array(outcome_test)\n",
    "df_distributions, df_results = get_stats_with_ci(outcome_test_array, all_predictions_test, 'test_set_results', optimal_threshold) #(y_label, y_pred, label, optimal_threshold, nsamples=2000):\n",
    "##optimal threshold: reuse the one computed on the train dataset\n",
    "##label: index of the dataframe, can be \"external radiomics results\"\n",
    "##returns a dataframe with auc accuracy precision recall f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bdb333-b8f8-4a9b-88a0-b78403f1b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb884e47-6363-4a78-a80d-501d8d094506",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test_binary = (np.array(all_predictions_test) > optimal_threshold).astype(int)\n",
    "\n",
    "cm = sklearn.metrics.confusion_matrix(y_true=outcome_test_array, y_pred=predictions_test_binary, normalize='true')\n",
    "disp = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rfe.classes_)\n",
    "disp.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb317f-3397-4a0d-8696-ecd0a6997e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"RFE Classifier\"\n",
    "fpr, tpr, thresholds = sklearn.metrics.roc_curve(outcome_test, all_predictions_test)\n",
    "roc_auc = sklearn.metrics.auc(fpr, tpr)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "ax.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.3f})', color='purple',)\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier (AUC = 0.500)')\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.0])\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('ROC Curve', fontsize=14)\n",
    "ax.legend(loc=\"lower right\", fontsize=15)\n",
    "ax.grid(alpha=1)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b5e11b-7f94-489b-b498-66a484462343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#auc_values, text_auc_summary, upper_lower_ci, mean_tpr = get_ci_for_auc(outcome_test_array, all_predictions_test)\n",
    "\n",
    "#perhaps this could be used to create CI around the ROC curve? No luck so far though, seems like I'd have to set up a new function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30addca7-16fc-426b-b2d3-ff8d90ddf1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc7e4f0-383b-474a-a705-0de5a6490210",
   "metadata": {},
   "source": [
    "#### SHAP VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19f059d-4bd9-44f1-a929-447ea958e878",
   "metadata": {},
   "outputs": [],
   "source": [
    "X100 = shap.utils.sample(reduced_features_train, 100) # I am not yet sure what the optimal number for distribution is. Standard (explained in documentation is 100.\n",
    "explainer_xgb = shap.Explainer(rfe.estimator_, X100) #This utilises the rfe xgboost with 10 features\n",
    "shap_values_xgb = explainer_xgb(reduced_features_train) #based on training dataset of model, since that is what controls final model architecture\n",
    "shap.plots.beeswarm(shap_values_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cbf32b-afe4-41d4-bd04-c00ddf24f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random_case = random.randint(0, len(reduced_features_train + 1))\n",
    "print(\"case index: \" + str(random_case))\n",
    "\n",
    "shap.plots.waterfall(shap_values_xgb[random_case])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f03305e-8f8d-4c5f-bedb-0f5b6d87da70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
